{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "三大爬虫类型\n",
    "- 通用爬虫\n",
    "- 聚焦爬虫\n",
    "---\n",
    "以下介绍通用爬虫。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "传教士.html保存成功。\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "if __name__ == \"__main__\":\n",
    "    # 指定url\n",
    "    url = 'https://www.sogou.com/web'\n",
    "    # UA 伪装：把 User-Agent 封装到一个字典中\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36'\n",
    "    }\n",
    "    # 处理 url 携带的参数：封装到字典中\n",
    "    kw = input('enter a word:')\n",
    "    param = {\n",
    "        'query':kw\n",
    "    }\n",
    "    # 发起请求\n",
    "    response = requests.get(url=url, params=param, headers=headers)\n",
    "    # 获取响应数据\n",
    "    page_text = response.text\n",
    "    # 持久化存储\n",
    "    fileName = kw + '.html'\n",
    "    with open(fileName, 'w', encoding='utf-8') as fp:\n",
    "        fp.write(page_text)\n",
    "    print(fileName + '保存成功。')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'errno': 0, 'data': [{'k': 'Yes', 'v': '[计]是'}, {'k': 'yes', 'v': 'adv. 是; （表示刚想起某事）哦; （表示不相信某人所言）真的吗; （鼓励某人继续讲）往下说 n'}, {'k': 'YES', 'v': 'abbr. Youth Education Systems 青少年教育系统; Youth Educa'}, {'k': 'Yes?', 'v': '什么事？'}, {'k': 'yeses', 'v': 'n. 表示同意的答覆，表示同意的人( yes的名词复数 )'}]}\n",
      "yes.json保存成功。\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "if __name__ == \"__main__\":\n",
    "    # 指定url\n",
    "    url = 'https://fanyi.baidu.com/sug'\n",
    "    # UA 伪装：把 User-Agent 封装到一个字典中\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36'\n",
    "    }\n",
    "    # 处理 url 携带的参数：封装到字典中\n",
    "    kw = input('enter a word:')\n",
    "    param = {\n",
    "        'kw':kw\n",
    "    }\n",
    "    # 发起请求\n",
    "    response = requests.post(url=url, data=param, headers=headers)\n",
    "    # 获取响应数据：json()方法返回的是 obj(如果确认服务器返回的是 json 类型才能使用)\n",
    "    dic_obj = response.json()\n",
    "    print(dic_obj)\n",
    "    # 持久化存储\n",
    "    fp = open('./' + kw + '.json', 'w', encoding='utf-8')\n",
    "    json.dump(dic_obj, fp=fp, ensure_ascii=False)\n",
    "    fp.close()\n",
    "    print(kw + '.json' + '保存成功。')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "douban.json保存成功。\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "if __name__ == \"__main__\":\n",
    "    # 指定url\n",
    "    # url = 'https://m.douban.com/rexxar/api/v2/movie/recommend'\n",
    "    # 排行榜+电影类型（下拉请求才能成功）\n",
    "    url = 'https://movie.douban.com/j/chart/top_list'\n",
    "    # UA 伪装：把 User-Agent 封装到一个字典中\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36'\n",
    "    }\n",
    "    # 处理 url 携带的参数：封装到字典中\n",
    "    # param = {\n",
    "    #     'refresh': '0',\n",
    "    #     'start': '20',\n",
    "    #     'count': '20',\n",
    "    #     'selected_categories': '{\"类型\":\"喜剧\"}',\n",
    "    #     'uncollect': 'false',\n",
    "    #     'tags': '喜剧'\n",
    "    # }\n",
    "    param = {\n",
    "        'action': '',\n",
    "        'start': '0',\n",
    "        'limit': '20',\n",
    "        'type': '24',\n",
    "        'interval_id': '100:90'\n",
    "    }\n",
    "    # 发起请求\n",
    "    response = requests.get(url=url, params=param, headers=headers)\n",
    "    # 获取响应数据：json()方法返回的是 obj(如果确认服务器返回的是 json 类型才能使用)\n",
    "    dic_obj = response.json()\n",
    "    # print(dic_obj)\n",
    "    # 持久化存储\n",
    "    fp = open('./douban.json', 'w', encoding='utf-8')\n",
    "    json.dump(dic_obj, fp=fp, ensure_ascii=False)\n",
    "    fp.close()\n",
    "    print('douban.json' + '保存成功。')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"Table\":[{\"rowcount\":41}],\"Table1\":[{\"rownum\":1,\"storeName\":\"广州\",\"addressDetail\":\"广州路104号\",\"pro\":\"24小时,Wi-Fi,点唱机,店内参观,礼品卡\",\"provinceName\":\"江苏省\",\"cityName\":\"南京市\"},{\"rownum\":2,\"storeName\":\"购书中心\",\"addressDetail\":\"天河路123号广州购书中心负一层\",\"pro\":\"Wi-Fi,点唱机,礼品卡\",\"provinceName\":\"广东省\",\"cityName\":\"广州市\"},{\"rownum\":3,\"storeName\":\"大学城\",\"addressDetail\":\"小谷围广州大学城广州大学生活区商业中心首层A1028及二层A2009\",\"pro\":\"24小时,Wi-Fi,点唱机,店内参观,礼品卡\",\"provinceName\":\"广东省\",\"cityName\":\"广州市\"},{\"rownum\":4,\"storeName\":\"车站\",\"addressDetail\":\"环西路159号广州火车站内首层候车室大厅右侧\",\"pro\":\"Wi-Fi,礼品卡\",\"provinceName\":\"广东省\",\"cityName\":\"广州市\"},{\"rownum\":5,\"storeName\":\"佳润\",\"addressDetail\":\"广州大道北1419号佳润广场首层\",\"pro\":\"Wi-Fi,店内参观,礼品卡\",\"provinceName\":\"广东省\",\"cityName\":\"广州市\"},{\"rownum\":6,\"storeName\":\"南站三\",\"addressDetail\":\"广州南站一楼到达层餐厅\",\"pro\":\"精选店,礼品卡\",\"provinceName\":\"广东省\",\"cityName\":\"广州市\"},{\"rownum\":7,\"storeName\":\"广州南站肯德基\",\"addressDetail\":\"广州南站肯德基一店\",\"pro\":\"礼品卡\",\"provinceName\":\"广东省\",\"cityName\":\"广州市\"},{\"rownum\":8,\"storeName\":\"梅花园\",\"addressDetail\":\"广州大道北28号一层\",\"pro\":\"24小时,Wi-Fi,店内参观,礼品卡\",\"provinceName\":\"广东省\",\"cityName\":\"广州市\"},{\"rownum\":9,\"storeName\":\"大剧院\",\"addressDetail\":\"珠江新城珠江西路一号广州大剧院多功能厅一层A01-1\",\"pro\":\"Wi-Fi,店内参观,礼品卡\",\"provinceName\":\"广东省\",\"cityName\":\"广州市\"},{\"rownum\":10,\"storeName\":\"金沙广场\",\"addressDetail\":\"胶州市阜安办事处广州路东、兖州路北侧一层的房产\",\"pro\":\"Wi-Fi,点唱机,店内参观,礼品卡\",\"provinceName\":\"山东省\",\"cityName\":\"青岛市\"}]}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "if __name__ == \"__main__\":\n",
    "    # 指定url\n",
    "    # op=keyword 不能忽略\n",
    "    url = 'http://www.kfc.com.cn/kfccda/ashx/GetStoreList.ashx'\n",
    "    # UA 伪装：把 User-Agent 封装到一个字典中\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36'\n",
    "    }\n",
    "    # 处理 url 携带的参数：封装到字典中\n",
    "    param = {\n",
    "        'op': 'keyword',\n",
    "        'cname': '',\n",
    "        'pid': '',\n",
    "        'keyword': '广州',\n",
    "        'pageIndex': '1',   # 页数\n",
    "        'pageSize': '10'\n",
    "    }\n",
    "    # 发起请求\n",
    "    response = requests.post(url=url, params=param, headers=headers)\n",
    "    # 获取响应数据\n",
    "    page_text = response.text\n",
    "    print(page_text)\n",
    "    # 持久化存储\n",
    "    # fp = open('./douban.json', 'w', encoding='utf-8')\n",
    "    # json.dump(dic_obj, fp=fp, ensure_ascii=False)\n",
    "    # fp.close()\n",
    "    # print('douban.json' + '保存成功。')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 药监局案例，先已有反爬虫机制，无法在网页抓包\n",
    "import requests\n",
    "import json\n",
    "if __name__ == \"__main__\":\n",
    "    # 指定url\n",
    "    url = 'xxx'\n",
    "    # UA 伪装：把 User-Agent 封装到一个字典中\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36'\n",
    "    }\n",
    "    # 处理 url 携带的参数：封装到字典中\n",
    "    data = {\n",
    "        'page': 1,\n",
    "        'xxx': '',\n",
    "        'xxxx': ''\n",
    "    }\n",
    "    # 发起请求\n",
    "    json_ids = requests.post(url=url, data=data, headers=headers)\n",
    "    # 获取响应数据\n",
    "    id_list = []    # 存储企业的 id\n",
    "    all_data_list = []      # 存储所有企业的详情数据\n",
    "    for dic in json_ids['list']:\n",
    "        id_list.append(dic['ID'])\n",
    "    print(id_list)  # 批量获取 id，得到一个企业 id 的数组\n",
    "    # 获取企业详情信息\n",
    "    post_url = 'xxxxx'\n",
    "    for id in id_list:\n",
    "        data = {\n",
    "            'id': id\n",
    "        }\n",
    "        detail_json = requests.post(url=post_url, headers=headers, data=data).json()\n",
    "        print(detail_json, '-----------ending----------')\n",
    "    # 持久化存储\n",
    "    fp = open('./allData.json', 'w', encoding='utf-8')\n",
    "    json.dump(all_data_list, fp=fp, ensure_ascii=False)\n",
    "    print('结束。')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 药监局案例，先已有反爬虫机制，无法在网页抓包\n",
    "# 代码优化，将 327 页的所有分页数据都爬出来\n",
    "import requests\n",
    "import json\n",
    "if __name__ == \"__main__\":\n",
    "    # 指定url\n",
    "    url = 'xxx'\n",
    "    # UA 伪装：把 User-Agent 封装到一个字典中\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36'\n",
    "    }\n",
    "    id_list = []    # 存储企业的 id\n",
    "    all_data_list = []      # 存储所有企业的详情数据\n",
    "    # 处理 url 携带的参数：封装到字典中\n",
    "    for page in range(1, 327):\n",
    "        page = str(page)\n",
    "        data = {\n",
    "            'page': page,\n",
    "            'xxx': '',\n",
    "            'xxxx': ''\n",
    "        }\n",
    "        # 发起请求\n",
    "        json_ids = requests.post(url=url, data=data, headers=headers)\n",
    "        # 获取响应数据\n",
    "        for dic in json_ids['list']:\n",
    "            id_list.append(dic['ID'])\n",
    "        print(id_list)  # 批量获取 id，得到一个企业 id 的数组\n",
    "    # 获取企业详情信息\n",
    "    post_url = 'xxxxx'\n",
    "    for id in id_list:\n",
    "        data = {\n",
    "            'id': id\n",
    "        }\n",
    "        detail_json = requests.post(url=post_url, headers=headers, data=data).json()\n",
    "        print(detail_json, '-----------ending----------')\n",
    "    # 持久化存储\n",
    "    fp = open('./allData.json', 'w', encoding='utf-8')\n",
    "    json.dump(all_data_list, fp=fp, ensure_ascii=False)\n",
    "    print('结束。')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下介绍聚焦爬虫。\n",
    "聚焦爬虫：爬取页面中指定的页面内容。\n",
    "\n",
    "数据解析分类：\n",
    "- 正则\n",
    "- bs4\n",
    "- xpath(***)\n",
    "\n",
    "数据解析原理概述：\n",
    "- 解析的局部的文本内容都会在标签之间或者标签对应的属性中进行存储\n",
    "1. 进行指定标签的定位\n",
    "2. 标签或者标签对应的属性中存储的数据值进行提取（解析）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "if __name__ == '__main__':\n",
    "    # 指定url\n",
    "    url = 'https://i0.hdslb.com/bfs/face/b6b42a42d0bd0145309d93fd23708da538ef696c.jpg@96w_96h.webp'\n",
    "    # content 返回的是二进制的图片数据\n",
    "    # text（字符串）content（二进制）json()（对象）\n",
    "    img_data = requests.get(url=url).content\n",
    "\n",
    "    with open('./pic_found.jpg', 'wb') as fp:\n",
    "        fp.write(img_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161934-1673252374e36d.jpg 下载成功！\n",
      "231950-1659453590cc8e.jpg 下载成功！\n",
      "223220-166161074052ed.jpg 下载成功！\n",
      "005404-1673196844ba1d.jpg 下载成功！\n",
      "005245-16731967650454.jpg 下载成功！\n",
      "004236-1673196156a356.jpg 下载成功！\n",
      "004827-16731101078ec8.jpg 下载成功！\n",
      "005542-1673024142cc0c.jpg 下载成功！\n",
      "005104-1673023864a712.jpg 下载成功！\n",
      "212516-1566653116f355.jpg 下载成功！\n",
      "005100-1592412660f973.jpg 下载成功！\n",
      "224716-16191892361adb.jpg 下载成功！\n",
      "001935-16159115757f04.jpg 下载成功！\n",
      "171501-16710093010818.jpg 下载成功！\n",
      "233543-16713777435ecb.jpg 下载成功！\n",
      "000930-1661184570f815.jpg 下载成功！\n",
      "005852-16684451324c3b.jpg 下载成功！\n",
      "005119-1670086279a31f.jpg 下载成功！\n",
      "000351-16727618319e3d.jpg 下载成功！\n",
      "113511-16715937115f7b.jpg 下载成功！\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "import os\n",
    "if __name__ == '__main__':\n",
    "    # 创建一个文件夹，保持所有图片\n",
    "    if not os.path.exists('./爬取的图片'):\n",
    "        os.mkdir('./爬取的图片')\n",
    "    # 指定url\n",
    "    url = 'https://pic.netbian.com/'\n",
    "    # UA 伪装：把 User-Agent 封装到一个字典中\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36'\n",
    "    }\n",
    "    # 使用通用爬虫对 url 对应的一整个页面进行爬取\n",
    "    page_text = requests.get(url=url, headers=headers).text\n",
    "    # 使用聚焦爬虫将页面中所有的图片进行解析/提取\n",
    "    ex = '<img src=\"(.*?)\" alt.*?>'\n",
    "    img_src_list = re.findall(ex, page_text, re.S)\n",
    "    # print(img_src_list)\n",
    "    for src in img_src_list:\n",
    "        # 拼接出一个完整的 url\n",
    "        src = 'https://pic.netbian.com/' + src\n",
    "        # 请求到了图片的二进制数据\n",
    "        img_data = requests.get(url=src, headers=headers).content  #二进制数据\n",
    "        # 生成图片名称\n",
    "        img_name = src.split('/')[-1]   # split 后的最后一个字符串\n",
    "        # 图片存储路径\n",
    "        imgPath = './爬取的图片/' + img_name\n",
    "        with open(imgPath, 'wb') as fp:\n",
    "            fp.write(img_data)\n",
    "            print(img_name, '下载成功！')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "230702-16725856220593.jpg 下载成功！\n",
      "230412-1672585452237b.jpg 下载成功！\n",
      "223603-1672583763e530.jpg 下载成功！\n",
      "165525-16727361253071.jpg 下载成功！\n",
      "122710-1672892830c501.jpg 下载成功！\n",
      "003302-167293638201be.jpg 下载成功！\n",
      "162124-1670142084496f.jpg 下载成功！\n",
      "004105-1672418465f933.jpg 下载成功！\n",
      "003915-16724183551704.jpg 下载成功！\n",
      "003750-1672418270dc6f.jpg 下载成功！\n",
      "113958-1535254798fc1c.jpg 下载成功！\n",
      "003604-1671986164e3f8.jpg 下载成功！\n",
      "003442-16665428829bab.jpg 下载成功！\n",
      "150422-16722974628504.jpg 下载成功！\n",
      "150208-1672297328e968.jpg 下载成功！\n",
      "144739-16722964591690.jpg 下载成功！\n",
      "144556-167229635663e1.jpg 下载成功！\n",
      "144408-1672296248118a.jpg 下载成功！\n",
      "174216-16722205361a64.jpg 下载成功！\n",
      "173740-1672220260f128.jpg 下载成功！\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "import os\n",
    "if __name__ == '__main__':\n",
    "    # 创建一个文件夹，保持所有图片\n",
    "    if not os.path.exists('./爬取的图片'):\n",
    "        os.mkdir('./爬取的图片')\n",
    "    # UA 伪装：把 User-Agent 封装到一个字典中\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36'\n",
    "    }\n",
    "    # 设置一个通用的 url 模板\n",
    "    url = 'https://pic.netbian.com/' + 'index_%d.html'\n",
    "    for pageNum in range(3, 4):\n",
    "        # 对应页码的 url\n",
    "        new_url = format(url%pageNum)\n",
    "        # 使用通用爬虫对 url 对应的一整个页面进行爬取\n",
    "        page_text = requests.get(url=new_url, headers=headers).text\n",
    "    # 使用聚焦爬虫将页面中所有的图片进行解析/提取\n",
    "    ex = '<img src=\"(.*?)\" alt.*?>'\n",
    "    img_src_list = re.findall(ex, page_text, re.S)\n",
    "    # print(img_src_list)\n",
    "    for src in img_src_list:\n",
    "        # 拼接出一个完整的 url\n",
    "        src = 'https://pic.netbian.com/' + src\n",
    "        # 请求到了图片的二进制数据\n",
    "        img_data = requests.get(url=src, headers=headers).content  #二进制数据\n",
    "        # 生成图片名称\n",
    "        img_name = src.split('/')[-1]   # split 后的最后一个字符串\n",
    "        # 图片存储路径\n",
    "        imgPath = './爬取的图片/' + img_name\n",
    "        with open(imgPath, 'wb') as fp:\n",
    "            fp.write(img_data)\n",
    "            print(img_name, '下载成功！')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bs4 进行数据解析的原理：\n",
    "---\n",
    "- 数据解析的原理：\n",
    "1. 标签定位\n",
    "2. 提取标签、标签属性中存储的数据值\n",
    "---\n",
    "- bs4 数据解析的原理：\n",
    "1. 实例化一个 BeautifulSoup 对象，并且将页面源码数据加载到该对象中\n",
    "2. 通过调用 BeautifulSoup 对象中相关的属性或者方法进行标签定位和数据提取\n",
    "---\n",
    "- 环境安装：\n",
    "1. pip install bs4\n",
    "2. pip install lxml\n",
    "---\n",
    "- 如何实例化 BeautifulSoup 对象：\n",
    "    - from bs4 import BeautifulSoup\n",
    "    - 对象的实例化：\n",
    "        - 将本地的 html 文档中的数据加载到该对象中\n",
    "            - fp = open('./test.html', 'r', encoding='utf-8')\n",
    "            - soup = BeautifulSoup(fp, 'lxml')\n",
    "        - 将互联网上获取的页面源码加载到该对象中\n",
    "            - page_text = response.text\n",
    "            - soup = BeautifulSoup(page_text, 'lxml')\n",
    "    - 提供的用于数据解析的方法和属性：\n",
    "        - soup.tagName：返回的是文档中第一次出现的 tagName 对应的标签\n",
    "        - soup.find()：\n",
    "            - soup.find('tagName') 等同于 soup.tagName\n",
    "            - 属性定位\n",
    "                - soup.find('div', class_/id/attr='xxx')\n",
    "        - soup.find_all('tagName')：返回符合要求的所有标签（列表）\n",
    "    - select：\n",
    "        - select('某种选择器（类选择器/id选择器/标签选择器）')，返回的是一个列表。\n",
    "        - 层级选择器：\n",
    "            - soup.select('.contact > nav > a > div')：> 表示一个层级\n",
    "            - soup.select('.contact > nav  div')：空格表示多个层级\n",
    "    - 获取标签之间的文本数据：\n",
    "        - soup.a.text/string/get_text()\n",
    "        - text/get_text()：可以获取某一个标签中所有文本内容\n",
    "        - string：只可以获取该标签下直系的文本内容\n",
    "    - 获取标签中属性值：\n",
    "        - soup.tagName['href/class等等']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['content']\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "if __name__ == '__main__':\n",
    "    # 将本地的 html 文档中的数据加载到该对象中\n",
    "    fp = open('C:/Users/30675/Desktop/project/contact-me-animation/dist/index.html', 'r', encoding='utf-8')\n",
    "    soup = BeautifulSoup(fp, 'lxml')\n",
    "    # soup.tagName ------------------------------------------------------------------------------\n",
    "    # print(soup)\n",
    "    # print(soup.a)\n",
    "    # print(soup.div)\n",
    "    # soup.find('tagName') ----------------------------------------------------------------------\n",
    "    # print(soup.find('div')) # print(soup.div)\n",
    "    # class 不加 _ 就不是一个参数名称，而是一个关键字了\n",
    "    # print(soup.find('div', class_='content'))   # 第二个参数可以是：class_、id、attr\n",
    "    # soup.find_all('tagName') ------------------------------------------------------------------\n",
    "    # print(soup.find_all('p'))\n",
    "    # soup.select('类选择器/id选择器/标签选择器') -------------------------------------------------\n",
    "    # print(soup.select('.gmail'))\n",
    "    # print(soup.select('.contact > nav > a > div')[1])   # 取得第二个 div 标签\n",
    "    # print(soup.select('.contact > nav  div')[1])    # 等于上一行\n",
    "    # print(soup.select('.contact > nav  div')[1].text)\n",
    "    # print(soup.select('.contact > nav  div')[1].get_text())    # 等于上一行\n",
    "    # print(soup.select('.contact > nav div')[1].string)\n",
    "    print(soup.select('.contact > nav div')[1]['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "学而篇 爬取成功！\n",
      "为政篇 爬取成功！\n",
      "八佾篇 爬取成功！\n",
      "里仁篇 爬取成功！\n",
      "公冶长篇 爬取成功！\n",
      "雍也篇 爬取成功！\n",
      "述而篇 爬取成功！\n",
      "泰伯篇 爬取成功！\n",
      "子罕篇 爬取成功！\n",
      "乡党篇 爬取成功！\n",
      "先进篇 爬取成功！\n",
      "颜渊篇 爬取成功！\n",
      "子路篇 爬取成功！\n",
      "宪问篇 爬取成功！\n",
      "卫灵公篇 爬取成功！\n",
      "季氏篇 爬取成功！\n",
      "阳货篇 爬取成功！\n",
      "微子篇 爬取成功！\n",
      "子张篇 爬取成功！\n",
      "尧曰篇 爬取成功！\n"
     ]
    }
   ],
   "source": [
    "# 爬取西游记小说所有的章节标题和章节内容\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "if __name__ == '__main__':\n",
    "    # 指定url\n",
    "    url = 'https://www.shicimingju.com/book/lunyu.html'\n",
    "    # UA 伪装：把 User-Agent 封装到一个字典中\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36'\n",
    "    }\n",
    "    # 获取响应数据\n",
    "    page_text = requests.get(url=url, headers=headers).content  # text 有乱码，使用 content 解决乱码\n",
    "    # 在首页中解析出章节的标题和详情页的 url\n",
    "    # 1. 实例化 BeautifulSoup 对象，需要将页面源码数据加载到该对象中\n",
    "    soup = BeautifulSoup(page_text, 'lxml')\n",
    "    # 2. 解析章节标题和详情页的 url\n",
    "    li_list = soup.select('.book-mulu > ul > li')\n",
    "    fp = open('./Journey_to_the_West.txt', 'w', encoding='utf-8')\n",
    "    for li in li_list:\n",
    "        title = li.a.string\n",
    "        detail_url = 'https://m.shicimingju.com' + li.a['href']\n",
    "        # 对详情页发送请求，解析章节内容\n",
    "        detail_page_text = requests.get(url=detail_url, headers=headers).content  # text 有乱码，使用 content 解决乱码\n",
    "        # 解析出详情页中相关章节内容\n",
    "        detail_soup = BeautifulSoup(detail_page_text, 'lxml')\n",
    "        div_tag = detail_soup.find('div', class_='chapter_content')\n",
    "        # 解析到了章节的内容\n",
    "        content = div_tag.text\n",
    "        fp.write(title + ':' + content + '\\n')\n",
    "        print(title, '爬取成功！')\n",
    "    fp.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xpath 解析：最常用且最便捷高效的一种解析方式。通用性。\n",
    "- xpath 解析原理：\n",
    "    - 1. 实例化一个 etree 对象，且需要将被解析的页面源码数据加载到该对象中\n",
    "    - 2. 调用 etree 对象中的 xpath 方法结合着 xpath 表达式实现标签的定位和内容的捕获\n",
    "- 环境的安装：\n",
    "    - pip install lxml\n",
    "- 如何实例化一个 etree 对象\n",
    "    - 1. 将本地 html 文档中的源码数据加载到 etree 对象中：from lxml import etree\n",
    "        - etree.parse(filePath)\n",
    "    - 2. 可以将从互联网上获取的源码数据加载到该对象中\n",
    "        - etree.HTML('page_text')\n",
    "    - xpath('xpath 表达式')\n",
    "        - /：表示的是从根节点开始定位。表示的是一个层级。\n",
    "        - //：表示的是多个层级。可以表示从任意位置开始定位。\n",
    "        - 属性定位：//div[@class='content']，即 tag[@attrName=\"attrValue\"]\n",
    "        - 索引定位：//div[@class=\"content\"]/h1[1] 所有是从 1 开始的。\n",
    "        - 取文本：\n",
    "            - /text() 获取的是标签中直系的文本内容\n",
    "            - //text() 标签中非直系的文本内容（所有的文本内容）\n",
    "        - 取属性：\n",
    "            - /@attrName    ==>img/src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['kiwi_cute.jpg']\n"
     ]
    }
   ],
   "source": [
    "from lxml import etree\n",
    "from lxml import html\n",
    "if __name__ == '__main__':\n",
    "    parser = etree.HTMLParser(encoding=\"utf-8\") #parser：解析器。文件名+网页类型+解析器（指定编码）\n",
    "    # 实例化好了一个 etree 对象，且将被解析的源码加载到了该对象中\n",
    "    tree = etree.parse('C:/Users/30675/Desktop/project/contact-me-animation/dist/index.html', parser=parser)\n",
    "    # ---------------------------------------------------\n",
    "    # r = tree.xpath('/html/head/title')\n",
    "    # ---------------------------------------------------\n",
    "    # r = tree.xpath('/html/body/div/div/nav/a')\n",
    "    # r = tree.xpath('/html//a')    # 等同于上一行\n",
    "    # r = tree.xpath('//a')    # 等同于上一行\n",
    "    # ---------------------------------------------------\n",
    "    # r = tree.xpath('//div[@class=\"content\"]')\n",
    "    # r = tree.xpath('//div[@class=\"content\"]/h1')\n",
    "    # r = tree.xpath('//div[@class=\"content\"]/h1[1]')\n",
    "    # ---------------------------------------------------\n",
    "    # r = tree.xpath('//div[@class=\"title\"]/text()')\n",
    "    # r = tree.xpath('//div[@class=\"title\"]//text()')\n",
    "    # r = tree.xpath('//div[@class=\"content\"]/h1[1]/text()')\n",
    "    # r = tree.xpath('//a[@class=\"twitter\"]/div[@class=\"content\"]/text()')\n",
    "    # r = tree.xpath('//a[@class=\"twitter\"]/div[@class=\"content\"]//text()')\n",
    "    # ---------------------------------------------------\n",
    "    r = tree.xpath('//div[@class=\"contact\"]/main//img/@src')\n",
    "\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 需求：爬取 58 二手房中的房源信息\n",
    "import requests\n",
    "from lxml import etree\n",
    "if __name__ == \"__main__\":\n",
    "    headers = {\n",
    "        'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36'\n",
    "    }\n",
    "    # 爬取到页面源码\n",
    "    url = 'https://bj.58.com/ershoufang/?PGTID=0d100000-0000-12fa-c745-f7b568ceab87'\n",
    "    page_text = requests.get(url=url, headers=headers).text\n",
    "    # 数据解析\n",
    "    tree = etree.HTML(page_text)\n",
    "    # 存储的是 h3 标签的 title\n",
    "    title_list = tree.xpath('//section[@class=\"list\"]/div[@class=\"property\"]/a/div[@class=\"property-content\"]/div[@class=\"property-content-detail\"]//h3/@title')\n",
    "    print(title_list)\n",
    "    fp = open('58.txt', 'w', encoding='utf-8')\n",
    "    for title in title_list:\n",
    "        print(title)\n",
    "        fp.write(title + '\\n')\n",
    "    fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 需求：爬取图片并保存\n",
    "import requests\n",
    "from lxml import etree\n",
    "if __name__ == \"__main__\":\n",
    "    headers = {\n",
    "        'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36'\n",
    "    }\n",
    "    # 爬取到页面源码\n",
    "    url = 'https://pic.netbian.com/4kmeinv/'\n",
    "    response = requests.get(url=url, headers=headers)\n",
    "    # 手动设置响应数据的编码格式\n",
    "    response.encoding = 'gbk'\n",
    "    page_text = response.text\n",
    "    # 数据解析\n",
    "    tree = etree.HTML(page_text)\n",
    "    img_list = tree.xpath('//ul[@class=\"clearfix\"]/li/a/img')\n",
    "    # print(img_list)\n",
    "    # 创建文件夹\n",
    "    if not os.path.exists('./pic'):\n",
    "        os.mkdir('./pic')\n",
    "    for img in img_list:\n",
    "        # 因为定位到的列表中只有一个元素，因此要加 [0]\n",
    "        img_src = 'https://pic.netbian.com/' + img.xpath('./@src')[0]\n",
    "        img_name = img.xpath('./@alt')[0] + '.jpg'\n",
    "        # 通用处理中文乱码的解决方法\n",
    "        # img_name = img_name.encode('iso-8859-1').decode('gbk')\n",
    "        #请求图片进行持久化存储\n",
    "        img_data = requests.get(url=img_src, headers=headers).content\n",
    "        img_path = 'pic/' + img_name\n",
    "        with open(img_path, 'wb') as fp:\n",
    "            fp.write(img_data)\n",
    "            print(img_name, '下载成功！')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 项目需求：解析出所有城市名称 http://www.aqistudy.cn/historydata/\n",
    "import requests\n",
    "from lxml import etree\n",
    "if __name__ == \"__main__\":\n",
    "    headers = {\n",
    "        'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36'\n",
    "    }\n",
    "    # 爬取到页面源码\n",
    "    url = 'http://www.aqistudy.cn/historydata/'\n",
    "    response = requests.get(url=url, headers=headers)\n",
    "    page_text = response.text\n",
    "    # 数据解析\n",
    "    tree = etree.HTML(page_text)\n",
    "    ##################### 方法一 #####################\n",
    "    hot_list = tree.xpath('//div[@class=\"hot\"]//ul/li')\n",
    "    all_city_name = []\n",
    "    # 解析到了热门城市所有名称\n",
    "    for li in hot_list:\n",
    "        hot_city_name = li.xpath('./a/text()')[0]\n",
    "        all_city_name.append(hot_city_name)\n",
    "    print(all_city_name, len(all_city_name))\n",
    "    # 数据解析\n",
    "    hot_list = tree.xpath('//div[@class=\"bottom\"]/ul/div[2]/li')\n",
    "    all_city_name = []\n",
    "    # 解析到了全部城市所有名称\n",
    "    for li in hot_list:\n",
    "        city_name = li.xpath('./a/text()')[0]   \n",
    "        all_city_name.append(city_name)\n",
    "    print(all_city_name, len(all_city_name))\n",
    "    ##################### 方法二 #####################\n",
    "    # a_list = tree.xpath('//div[@class=\"hot\"]//ul/li/a | //div[@class=\"bottom\"]/ul/div[2]/li/a')\n",
    "    # all_city_name = []\n",
    "    # for a in a_list:\n",
    "    #     city_name = a.xpath('./text()')[0]\n",
    "    #     all_city_name.append(city_name)\n",
    "    # print(all_city_name, len(all_city_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 项目需求：下载简历模板 http://sc.chinaz.com/jianli/free.html\n",
    "import requests\n",
    "from lxml import etree\n",
    "import json\n",
    "if __name__ == \"__main__\":\n",
    "    headers = {\n",
    "        'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36'\n",
    "    }\n",
    "    # 爬取到页面源码\n",
    "    url = 'http://sc.chinaz.com/jianli/free.html'\n",
    "    response = requests.get(url=url, headers=headers)\n",
    "    # response.encoding = 'gbk'\n",
    "    page_text = response.text\n",
    "    page_text = page_text.encode('iso-8859-1').decode('utf-8')\n",
    "    # 数据解析\n",
    "    tree = etree.HTML(page_text)\n",
    "    div_list = tree.xpath('//div[@id=\"main\"]/div/div')\n",
    "    # print(div_list)\n",
    "    for div in div_list:\n",
    "        href = div.xpath('./a/@href')[0]\n",
    "        # print(href)\n",
    "        # 爬取详情页源码\n",
    "        response_detail = requests.get(url=href, headers=headers)\n",
    "        detail_text = response_detail.text\n",
    "        # 详情页数据解析\n",
    "        detail_tree = etree.HTML(detail_text)\n",
    "        download_address = detail_tree.xpath('//ul[@class=\"clearfix\"]/li[3]/a/@href')\n",
    "        # print(download_address[0])\n",
    "        rar_name = href.split('/')[-1]  # split 最后一个字符串\n",
    "        rar_name = rar_name.split('.')[0]\n",
    "        rar_data = requests.get(url=download_address[0]).content\n",
    "\n",
    "        with open('./简历下载/' + rar_name + '.rar', 'wb') as fp:\n",
    "            fp.write(rar_data)\n",
    "            print(rar_name, '下载成功')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3a3fe59a4ae3b28e978716cbf9ec9812829918c0e269638a4f56e8387ef4c12a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
